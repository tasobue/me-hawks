{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "必要モジュールのインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install shortuuid keras numpy==1.14.3 pandas==0.24.2 Pillow==5.2.0 requests==2.20.0 --target train/module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習済みモデルのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('train/module')\n",
    "import utils\n",
    "\n",
    "weight = utils.download_weiths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定数の宣言"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('train/module')\n",
    "\n",
    "#S3に作成されるフォルダの名前\n",
    "PROJECT_NAME = 'sagemaker-with-keras-traing2deploy'\n",
    "TAGS = [{ 'Key': 'example.ProjectName', 'Value': PROJECT_NAME }]\n",
    "#tarinフォルダにある[train_xx.py]を指定する。xxの部分とVERSIONを一致させて使用する。\n",
    "VERSION = 'v2'\n",
    "#S3のバケット名　あらかじめ作成しておくこと\n",
    "BUCKET_NAME = f'sage-maker-201908132134'\n",
    "#S3からデータをダウンロードする際の設定（不使用）\n",
    "DATA_ROOT = f's3://{BUCKET_NAME}/{PROJECT_NAME}'\n",
    "#S3からデータをダウンロードする際の設定（不使用）\n",
    "TRAINS_DIR = f'{DATA_ROOT}/data/trains'\n",
    "#S3からデータをダウンロードする際の設定（不使用）\n",
    "TESTS_DIR = f'{DATA_ROOT}/data/tests'\n",
    "#S3に学習結果をアップロードする際の設定\n",
    "OUTPUTS_DIR = f'{DATA_ROOT}/outputs'\n",
    "#↓SageMakerコンソール ノートブック->Permissions and encryption->IAM ロール ARNのarnをコピペする\n",
    "ROLE = 'arn:aws:iam::902023299513:role/service-role/AmazonSageMaker-ExecutionRole-20190812T024057'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パラメータの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "import logging\n",
    "\n",
    "#train_xx.pyに渡す学習パラメータ\n",
    "# batch-size:バッチサイズ\n",
    "# epochs:エポック数\n",
    "# lr:学習係数\n",
    "# opt:オプティマイザー=最適化アルゴリズム\n",
    "# depth:NN(ニューラルネットワーク)の隠れ層の数\n",
    "# width:NNの各隠れ層のノードの数\n",
    "# validation-split:教師データを学習用とテスト用に分ける比率。学習：テスト=入力値：(1-入力値)\n",
    "# model-version:train_xx.pyのバージョン\n",
    "# weight-file:学習結果の重みファイル\n",
    "params = {\n",
    "    'batch-size': 1,\n",
    "    'epochs': 1,\n",
    "    'lr':0.1,\n",
    "    'opt':'sgd',\n",
    "    'depth': 16,\n",
    "    'width':8,\n",
    "    'validation-split': 0.1,\n",
    "    'model-version': VERSION,\n",
    "    'weight-file':'pretrained_models/weights.28-3.73.hdf5'\n",
    "}\n",
    "\n",
    "# 学習状況を計測する指標\n",
    "# CloudWotachに出力される\n",
    "metric_definitions = [\n",
    "    {'Name': 'train:acc', 'Regex': 'acc: (\\S+)'},\n",
    "    {'Name': 'train:mse', 'Regex': 'mean_squared_error: (\\S+)'},\n",
    "    {'Name': 'train:mae', 'Regex': 'mean_absolute_error: (\\S+)'},\n",
    "    {'Name': 'train:top-k', 'Regex': 'top_k_categorical_accuracy: (\\S+)'},\n",
    "    {'Name': 'valid:acc', 'Regex': 'val_acc: (\\S+)'},\n",
    "    {'Name': 'valid:mse', 'Regex': 'val_mean_squared_error: (\\S+)'},\n",
    "    {'Name': 'valid:mae', 'Regex': 'val_mean_absolute_error: (\\S+)'},\n",
    "    {'Name': 'valid:top-k', 'Regex': 'val_top_k_categorical_accuracy: (\\S+)'},\n",
    "]\n",
    "\n",
    "#学習時に実行するTensorFlowのバージョンやインスタンスタイプの設定など\n",
    "estimator = TensorFlow(\n",
    "    role=ROLE,\n",
    "    source_dir='train',\n",
    "    entry_point=f'train_{VERSION}.py',\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.p2.xlarge',\n",
    "    framework_version='1.12.0',\n",
    "    py_version='py3',\n",
    "    script_mode=True,\n",
    "    hyperparameters=params,\n",
    "    dependencies=['train/module'],\n",
    "    output_path=OUTPUTS_DIR,\n",
    "    container_log_level=logging.INFO,\n",
    "    metric_definitions=metric_definitions,\n",
    "    tags=TAGS\n",
    ")\n",
    "\n",
    "#学習に渡すデータの設定（不使用）\n",
    "#設定しておかないとエラーになるため一応設定している\n",
    "inputs = {'train': TRAINS_DIR, 'test': TESTS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shortuuid\n",
    "\n",
    "uuid = shortuuid.ShortUUID().random(length=8)\n",
    "# estimator.fit(job_name=f'{PROJECT_NAME}-{VERSION}-s-{uuid}', inputs=inputs)\n",
    "estimator.fit(job_name=f'{PROJECT_NAME}-{VERSION}-s-{uuid}', inputs=inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習結果のモデルのダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "#estimator = TensorFlow.attach(tuner.best_training_job())\n",
    "#print(tuner.best_training_job())\n",
    "\n",
    "url = urlparse(estimator.model_data)\n",
    "s3_root_dir = '/'.join(url.path.split('/')[:-2])[1:]\n",
    "model_s3path = s3_root_dir + '/output/model.tar.gz'\n",
    "#output_s3path = s3_root_dir + '/output/output.tar.gz'\n",
    "model_filename = 'predict/model_v1.h5'\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket(url.netloc)\n",
    "\n",
    "print(model_s3path)\n",
    "bucket.download_file(model_s3path, 'predict/model.tar.gz')\n",
    "#bucket.download_file(output_s3path, 'predict/output.tar.gz')\n",
    "\n",
    "\n",
    "#with open('models/labels.pickle', mode='rb') as f:\n",
    "#    labels = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推論コンテナ用フォルダに解凍する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd predict; tar zxvf model.tar.gz;\n",
    "#!cd predict; tar tar zxvf output.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ダウンロードしたモデルの入力と出力を確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "\n",
    "# TensorFlowによるモデルのロード\n",
    "session = tf.keras.backend.get_session()\n",
    "tf_model = tf.saved_model.loader.load(session, [tag_constants.SERVING], 'predict/1');\n",
    "\n",
    "# input/outputのシグネチャ名確認\n",
    "model_signature = tf_model.signature_def['serving_default']\n",
    "input_signature = model_signature.inputs\n",
    "output_signature = model_signature.outputs\n",
    "\n",
    "for k in input_signature.keys():\n",
    "    print(k)\n",
    "for k in output_signature.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推論コンテナ用に再パッケージ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd predict; tar zcvf model.tar.gz 1 code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S3に推論用ソースをアップロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "predict_model_url = urllib.parse.urlparse(f'{estimator.output_path}/{estimator.latest_training_job.job_name}/predict/model.tar.gz')\n",
    "bucket.upload_file('predict/model.tar.gz', predict_model_url.path[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※estimatorのプロパティを使用しない場合のアップロードは↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import urllib\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "#estimator = TensorFlow.attach(tuner.best_training_job())\n",
    "#print(tuner.best_training_job())\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('sage-maker-201908132134')\n",
    "\n",
    "predict_model_url = urllib.parse.urlparse(f's3://sage-maker-201908132134/sagemaker-with-keras-traing2deploy/outputs/sagemaker-with-keras-traing2deploy-v1-s-W9nCEBS5/predict/model.tar.gz')\n",
    "bucket.upload_file('predict/model.tar.gz', predict_model_url.path[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "エンドポイントの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow.serving import Model\n",
    "\n",
    "tensorflow_serving_model = Model(model_data=f'{predict_model_url.scheme}://{predict_model_url.hostname}{predict_model_url.path}',\n",
    "                                 role=ROLE,\n",
    "                                 framework_version='1.13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = tensorflow_serving_model.deploy(initial_instance_count=1,\n",
    "                                            instance_type='ml.t2.large',\n",
    "                                            tags=TAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_model_url\n",
    "estimator.latest_training_job.job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "画像認識モデルを動作確認する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(送信側の作業)Base64エンコードの画像をJSONにシリアライズする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "def encodeToBase64(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        img_base64 = base64.b64encode(f.read())\n",
    "\n",
    "    return img_base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "b64img = encodeToBase64('./train/data/ogurishun.jpg')\n",
    "dcded_b64img = b64img.decode('utf-8')\n",
    "query = {'image':dcded_b64img }\n",
    "param = json.dumps(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "画像送信"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('sagemaker-runtime')\n",
    "res = client.invoke_endpoint(\n",
    "    EndpointName=predictor.endpoint,\n",
    "    Body=param,\n",
    "    ContentType='application/json',\n",
    "    Accept='application/json'\n",
    ")\n",
    "body = res['Body'].read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "画損認識の結果を取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = body.decode('utf-8')\n",
    "x = json.loads(r)\n",
    "x = x[0]['prob']\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm --target train/module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ./train; python ./evaluate_appa_real.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
